{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28df0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registros diarios de temperatura con al menos un dato válido: 224\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pyspark\n",
    "\n",
    "if \"sc\" in globals ():\n",
    "    sc.stop()\n",
    "\n",
    "#Inicializar Spark\n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "\n",
    "#Cargar datos archivo\n",
    "data = sc.textFile('calidad_aire_datos_meteo_mes.csv')\n",
    "\n",
    "header = data.first()\n",
    "data_no_header = data.filter(lambda row: row != header)\n",
    "\n",
    "magnitud = 3\n",
    "\n",
    "parded_data = data_no_header.map(lambda line:line.split(';'))\n",
    "       \n",
    "\n",
    "rdd_temp = parded_data.filter(lambda cols: cols[magnitud] == \"83\")\n",
    "\n",
    "\n",
    "def tiene_dato_valido(cols):\n",
    "    return 'V' in cols\n",
    "\n",
    "rdd_filtrado = rdd_temp.filter(tiene_dato_valido)\n",
    "\n",
    "# 5. Contar los registros resultantes \n",
    "total_registros_validos = rdd_filtrado.count()\n",
    "\n",
    "print(f\"Registros diarios de temperatura con al menos un dato válido: {total_registros_validos}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1425dd50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-01: 11.7 °C\n",
      "2026-02-02: 12.2 °C\n",
      "2026-02-03: 9.8 °C\n",
      "2026-02-04: 11.3 °C\n",
      "2026-02-05: 15.5 °C\n",
      "2026-02-06: 10.3 °C\n",
      "2026-02-07: 9.0 °C\n",
      "2026-02-08: 12.7 °C\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "# 1. Inicializar Spark (reiniciando si ya existe)\n",
    "if \"sc\" in globals():\n",
    "    sc.stop()\n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "\n",
    "# 2. Cargar datos y quitar cabecera\n",
    "data = sc.textFile('calidad_aire_datos_meteo_mes.csv')\n",
    "header = data.first()\n",
    "rdd_datos = data.filter(lambda row: row != header).map(lambda line: line.split(';'))\n",
    "\n",
    "# 3. Filtrar solo la Magnitud 83 (Temperatura)\n",
    "rdd_temp = rdd_datos.filter(lambda cols: cols[3] == \"83\")\n",
    "\n",
    "# 4. Función simplificada para extraer (fecha, temperatura) de cada hora válida\n",
    "def extraer_temperaturas(cols):\n",
    "    # Formatear la fecha como YYYY-MM-DD (índices 5=Año, 6=Mes, 7=Día)\n",
    "    fecha = f\"{cols[5]}-{cols[6].zfill(2)}-{cols[7].zfill(2)}\"\n",
    "    \n",
    "    # Recorrer las 24 horas. Los valores van del índice 8 al 54 saltando de 2 en 2\n",
    "    for i in range(8, 56, 2):\n",
    "        valor = cols[i]\n",
    "        validacion = cols[i+1]\n",
    "        \n",
    "        # Si la hora es válida ('V') y no está vacía, devolvemos el par (fecha, temperatura)\n",
    "        if validacion == 'V' and valor.strip() != '':\n",
    "            temp_float = float(valor.replace(',', '.'))\n",
    "            yield (fecha, temp_float)  #Yield lo pone en formato listas expulsando los datos no validos\n",
    "\n",
    "# 5. Aplicar flatMap para obtener una lista gigante de todas las (fecha, temperatura) válidas\n",
    "rdd_pares = rdd_temp.flatMap(extraer_temperaturas)  #FlatMap se encarga él solo de coger todas esas listitas de cada estación y juntarlas en uno solo\n",
    "\n",
    "# 6. Agrupar por fecha y quedarse con la temperatura máxima de cada día\n",
    "rdd_max_diaria = rdd_pares.reduceByKey(max).sortByKey()\n",
    "\n",
    "# 7. Mostrar resultados\n",
    "for fecha, temp_max in rdd_max_diaria.collect():\n",
    "    print(f\"{fecha}: {temp_max} °C\")\n",
    "\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25d3f5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESTACIÓN CON MAYOR PRECIPITACIÓN POR DÍA:\n",
      "2026-02-01 | Municipio: 120 | Estación: 1 | Total: 20.8 mm\n",
      "2026-02-02 | Municipio: 161 | Estación: 1 | Total: 18.4 mm\n",
      "2026-02-03 | Municipio: 127 | Estación: 4 | Total: 7.6 mm\n",
      "2026-02-04 | Municipio: 45 | Estación: 2 | Total: 11.6 mm\n",
      "2026-02-05 | Municipio: 115 | Estación: 3 | Total: 30.8 mm\n",
      "2026-02-06 | Municipio: 67 | Estación: 1 | Total: 6.3 mm\n",
      "2026-02-07 | Municipio: 115 | Estación: 3 | Total: 21.8 mm\n",
      "2026-02-08 | Municipio: 120 | Estación: 1 | Total: 25.1 mm\n",
      "RÉCORD ABSOLUTO DEL PERIODO:\n",
      "Fecha: 2026-02-05 | Municipio: 115 | Estación: 3 | Total: 30.8 mm\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "# 1. Inicializar Spark\n",
    "if \"sc\" in globals():\n",
    "    sc.stop()\n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "\n",
    "# 2. Cargar datos y quitar cabecera\n",
    "data = sc.textFile('calidad_aire_datos_meteo_mes.csv')\n",
    "header = data.first()\n",
    "rdd_datos = data.filter(lambda row: row != header).map(lambda line: line.split(';'))\n",
    "\n",
    "# 3. Filtrar solo la Magnitud 89 (Precipitación)\n",
    "rdd_precip = rdd_datos.filter(lambda cols: cols[3] == \"89\")\n",
    "\n",
    "\n",
    "# 4. Función para calcular la suma de precipitación de una estación\n",
    "def calcular_precipitacion_diaria(cols):\n",
    "    municipio = cols[1]\n",
    "    estacion = cols[2]\n",
    "    # Formateamos la fecha a YYYY-MM-DD\n",
    "    fecha = f\"{cols[5]}-{cols[6].zfill(2)}-{cols[7].zfill(2)}\"\n",
    "    \n",
    "    suma_diaria = 0.0\n",
    "\n",
    "    # Recorremos las 24 horas (valores y validaciones)\n",
    "    for i in range(8, 56, 2):\n",
    "        valor = cols[i]\n",
    "        validacion = cols[i+1]\n",
    "        \n",
    "        # Solo sumamos si el dato es válido ('V') y no está vacío\n",
    "        if validacion == 'V' and valor.strip() != '':\n",
    "            suma_diaria += float(valor.replace(',', '.'))\n",
    "            \n",
    "    # Usamos yield para expulsar una tupla: (Clave, Valor)\n",
    "    # Clave -> fecha\n",
    "    # Valor -> (municipio, estacion, suma_diaria)\n",
    "    yield (fecha, (municipio, estacion, suma_diaria))\n",
    "\n",
    "\n",
    "\n",
    "# 5. Aplicar flatMap (al usar yield, flatMap \"desempaqueta\" lo que el generador produce)\n",
    "rdd_totales_diarios = rdd_precip.flatMap(calcular_precipitacion_diaria)\n",
    "\n",
    "\n",
    "# 6. Agrupar por fecha y quedarse con la estación que tenga la suma mayor ese día\n",
    "# Comparamos el índice 2 de los valores (que corresponde a suma_diaria)\n",
    "rdd_max_diaria = rdd_totales_diarios.reduceByKey(lambda a, b: a if a[2] >= b[2] else b)\n",
    "\n",
    "# Ordenamos por fecha\n",
    "rdd_max_diaria_ordenado = rdd_max_diaria.sortByKey()\n",
    "\n",
    "# 7. Recoger y mostrar el primer resultado\n",
    "resultados = rdd_max_diaria_ordenado.collect()\n",
    "\n",
    "print(\"ESTACIÓN CON MAYOR PRECIPITACIÓN POR DÍA:\")\n",
    "\n",
    "for fecha, datos in resultados:\n",
    "    municipio, estacion, precipitacion = datos\n",
    "    print(f\"{fecha} | Municipio: {municipio} | Estación: {estacion} | Total: {round(precipitacion, 2)} mm\")\n",
    "\n",
    "# 8. Calcular el récord absoluto de todo el periodo\n",
    "# La función max de Spark busca el máximo evaluando la función que le pasemos.\n",
    "# En este caso le decimos que mire en x[1][2], que es el total de precipitación.\n",
    "record_absoluto = rdd_max_diaria.max(key=lambda x: x[1][2])\n",
    "\n",
    "fecha_rec = record_absoluto[0]\n",
    "muni_rec, est_rec, prec_rec = record_absoluto[1]\n",
    "\n",
    "print(\"RÉCORD ABSOLUTO DEL PERIODO:\")\n",
    "print(f\"Fecha: {fecha_rec} | Municipio: {muni_rec} | Estación: {est_rec} | Total: {round(prec_rec, 2)} mm\")\n",
    "\n",
    "sc.stop()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2739a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPARATIVA DE TEMPERATURAS MEDIAS DIARIAS\n",
      "(Referencia: Mun 6 / Est 4  |  Comparada: Mun 5 / Est 2)\n",
      "------------------------------------------------------------\n",
      "Fecha: 2026-02-01 | Porcentaje: 129.04 %\n",
      "Fecha: 2026-02-02 | Porcentaje: 113.45 %\n",
      "Fecha: 2026-02-03 | Porcentaje: 114.98 %\n",
      "Fecha: 2026-02-04 | Porcentaje: 114.01 %\n",
      "Fecha: 2026-02-05 | Porcentaje: 112.09 %\n",
      "Fecha: 2026-02-06 | Porcentaje: 116.52 %\n",
      "Fecha: 2026-02-07 | Porcentaje: 131.91 %\n",
      "Fecha: 2026-02-08 | Porcentaje: 116.34 %\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "# 1. Inicializar Spark\n",
    "if \"sc\" in globals():\n",
    "    sc.stop()\n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "\n",
    "# 2. Cargar datos y quitar cabecera\n",
    "data = sc.textFile('calidad_aire_datos_meteo_mes.csv')\n",
    "header = data.first()\n",
    "rdd_datos = data.filter(lambda row: row != header).map(lambda line: line.split(';'))\n",
    "\n",
    "# 3. Filtrar Magnitud 83 (Temperatura)\n",
    "rdd_temp = rdd_datos.filter(lambda cols: cols[3] == \"83\")\n",
    "\n",
    "# 4. Función para extraer las medias de las dos estaciones concretas\n",
    "def extraer_medias_estaciones(cols):\n",
    "    # Convertimos a entero para evitar problemas con ceros a la izquierda (ej. \"006\" vs \"6\")\n",
    "    mun = int(cols[1])\n",
    "    est = int(cols[2])\n",
    "    \n",
    "    # Identificamos de qué estación se trata\n",
    "    if mun == 6 and est == 4:\n",
    "        tipo = 'referencia'\n",
    "    elif mun == 5 and est == 2:\n",
    "        tipo = 'comparada'\n",
    "    else:\n",
    "        # Si no es ninguna de las dos, devolvemos lista vacía para que flatMap la ignore\n",
    "        return []\n",
    "        \n",
    "    fecha = f\"{cols[5]}-{cols[6].zfill(2)}-{cols[7].zfill(2)}\"\n",
    "    \n",
    "    suma_temp = 0.0\n",
    "    horas_validas = 0\n",
    "    \n",
    "    # Recorremos las horas\n",
    "    for i in range(8, 56, 2):\n",
    "        valor = cols[i]\n",
    "        validacion = cols[i+1]\n",
    "        \n",
    "        if validacion == 'V' and valor.strip() != '':\n",
    "            suma_temp += float(valor.replace(',', '.'))\n",
    "            horas_validas += 1\n",
    "            \n",
    "    # Solo si hubo alguna hora válida ese día calculamos la media\n",
    "    if horas_validas > 0:\n",
    "        media_diaria = suma_temp / horas_validas\n",
    "        # Devolvemos la fecha, el tipo de estación y su media\n",
    "        return [(fecha, (tipo, media_diaria))]\n",
    "    \n",
    "    return []\n",
    "\n",
    "# 5. Aplicar la función a todo el RDD\n",
    "rdd_medias = rdd_temp.flatMap(extraer_medias_estaciones)\n",
    "\n",
    "# 6. Separar en dos RDDs diferentes\n",
    "# Nos quedamos con tuplas de (Fecha, Media) para cada RDD\n",
    "rdd_referencia = rdd_medias.filter(lambda x: x[1][0] == 'referencia').map(lambda x: (x[0], x[1][1]))\n",
    "rdd_comparada  = rdd_medias.filter(lambda x: x[1][0] == 'comparada').map(lambda x: (x[0], x[1][1]))\n",
    "\n",
    "# 7. Unir ambos RDDs por la fecha usando JOIN\n",
    "# El resultado será: (Fecha, (Media_Referencia, Media_Comparada))\n",
    "rdd_unido = rdd_referencia.join(rdd_comparada)\n",
    "\n",
    "# 8. Calcular el porcentaje y formatear\n",
    "def calcular_porcentaje(datos_unidos):\n",
    "    fecha = datos_unidos[0]\n",
    "    media_ref = datos_unidos[1][0]\n",
    "    media_comp = datos_unidos[1][1]\n",
    "    \n",
    "    # Evitamos dividir por cero por si la media de referencia fuera exactamente 0.0\n",
    "    if media_ref != 0:\n",
    "        porcentaje = (media_comp / media_ref) * 100\n",
    "    else:\n",
    "        porcentaje = 0.0\n",
    "        \n",
    "    return (fecha, porcentaje)\n",
    "\n",
    "# Aplicamos el cálculo y ordenamos por fecha\n",
    "rdd_resultado = rdd_unido.map(calcular_porcentaje).sortByKey()\n",
    "\n",
    "# 9. Recoger y mostrar resultados\n",
    "resultados = rdd_resultado.collect()\n",
    "\n",
    "print(\"COMPARATIVA DE TEMPERATURAS MEDIAS DIARIAS\")\n",
    "print(\"(Referencia: Mun 6 / Est 4  |  Comparada: Mun 5 / Est 2)\")\n",
    "print(\"-\" * 60)\n",
    "for fecha, porcentaje in resultados:\n",
    "    print(f\"Fecha: {fecha} | Porcentaje: {round(porcentaje, 2)} %\")\n",
    "\n",
    "# Detener Spark\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
